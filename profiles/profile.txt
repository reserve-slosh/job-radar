# Kandidatenprofil: Flemming Reese

## Person
- Name: Flemming Reese
- Wohnort: Köln
- Kontakt: +49 160 5659322 | flemming.reese@gmail.com
- LinkedIn: linkedin.com/in/flemming-reese

## Ausbildung
- M.Sc. Data Science, HAW Kiel, Abschluss 02/2025, Note 1,7 (Masterarbeit: 2,0)
- B.Sc. Sozio-Ökonomik, CAU Kiel, Schwerpunkt VWL + Statistik/Datenverarbeitung

## Berufserfahrung
- GEOMAR Helmholtz-Zentrum (10/2024–12/2025): Python-Automatisierung für magnetische Anomaliedetektion, Routenplanungsalgorithmen für AUVs, Ergebnisaufbereitung für interdisziplinäre Teams
- Freiberuflich (2024–2025): Automatisierte Event-Datenpipeline (Scraping mit BS4/Requests, Normalisierung, Deduplizierung, MySQL, Docker, Google Cloud, Hetzner); Troubleshooting und Redeployment einer fehlerhaft betriebenen Produktionsanwendung; Beratung Datenpipeline-Konzeption für Kunststoffrecycling-KMU
- Reese & Green Bauunternehmung (2022–2024): IT-Infrastruktur, Website, Administration

## Masterarbeit
Efficient Path Planning for Autonomous Underwater Vehicles — Optimizing Multi-Cluster Coverage Under Operational and Environmental Constraints. Vollständiges Python-Package (src-Layout, Module für Routing/Environment/IO/Visualisierung, Testsuite), Vergleich von Heuristiken und Metaheuristiken.

## Tech Stack
- **Sehr gut**: Python (Pandas, NumPy, Requests, BeautifulSoup, Matplotlib, pytest)
- **Gut**: SQL (MySQL, PostgreSQL, SQLite), Git, Docker, Linux (Ubuntu), Google Cloud
- **Grundkenntnisse**: Scikit-learn, PyTorch, FastAPI/Flask, R
- **Konzeptuell bekannt**: Spark, Kafka, verteilte Verarbeitung

## Stärken / Positionierung
- Data Engineering: ETL/ELT-Pipelines, Datenmodellierung, Automatisierung, Deployment
- Saubere Softwarearchitektur (Modularität, Testbarkeit, Wartbarkeit)
- End-to-End-Verantwortung: von Konzept bis Produktionsbetrieb
- Kommunikation mit nicht-technischen Stakeholdern
- Intrinsisch motiviert durch Infrastruktur und Systemwartung (eigener Homeserver)
- Schnelle Einarbeitung in neue Stacks sobald konkreter Kontext vorhanden

## Ziel
Junior Data Engineer oder Data Scientist, Einstieg, bevorzugt Köln/vor Ort, offen für Remote

## Ehrenamt
MUDDI Markt e.V. seit 06/2023: Kulturveranstaltungen, Vortragserfahrung (~100 Personen), Ehrung durch Ministerpräsident SH (2025)

---

# Stil-Referenz: Zwei echte Anschreiben von Flemming

## Anschreiben A — Qimia (formell, reflektiert, Lücken explizit genannt)
Einsatz bei: etablierten Unternehmen, Beratungen, Konzernen, traditionellerer Unternehmenskultur.

Sehr geehrte Damen und Herren,

ich habe meinen Master in Data Science im Februar 2025 abgeschlossen und suche nun den Einstieg in ein Umfeld, das mit echten Datenproblemen arbeitet. Qimia spricht mich an, weil das Unternehmen beides verbindet: technisch anspruchsvolle Projektarbeit und ein klares Interesse daran, Mitarbeiterinnen und Mitarbeiter weiterzuentwickeln.

Was mich im Laufe des Studiums am stärksten interessiert hat, war weniger die Modellierung als das Bauen: robuste Pipelines, saubere Systemarchitektur, Automatisierung von Prozessen, die vorher manuell oder gar nicht liefen. Data Engineering ist der Bereich, der das am direktesten erfüllt. Ich betreibe privat einen eigenen Homeserver, weil mich Deployment, Monitoring und Systemwartung intrinsisch motivieren und mir die Arbeit, Dinge zuverlässig im Hintergrund laufen zu lassen, echten Spaß macht.

In meiner Masterarbeit habe ich ein vollständiges Python-Package zur Routenoptimierung für autonome Unterwasserfahrzeuge entwickelt: mit eigenem src-Layout, getrennten Modulen für Routing, Environment, IO und Visualisierung sowie einer vollständigen Testsuite. Mehrere Solver-Familien wurden unter operativen und umweltbedingten Constraints systematisch bewertet. Das Package ist modular aufgebaut, sodass andere Forschende damit weiterarbeiten können.

Produktionserfahrung habe ich durch zwei freiberufliche Projekte für ein Medienunternehmen gesammelt, zu dem ich über ein Studienprojekt Kontakt aufgebaut hatte. Der wesentliche Auftrag war eine automatisierte Event-Datenpipeline: Scraping, Normalisierung, Deduplizierung, Persistierung in PostgreSQL, containerisiert mit Docker, betrieben in der Google Cloud und auf einem Hetzner-Server. Deployment, Monitoring und Wartung habe ich eigenständig verantwortet. Ein zweites Projekt bestand im Troubleshooting und Redeployment einer Anwendung, die fehlerhaft in Betrieb genommen worden war.

Am GEOMAR habe ich Python-basierte Auswertungsprozesse automatisiert und Ergebnisse für eine interdisziplinäre Forschungsgruppe aufbereitet, mit dem Anspruch, algorithmisch komplexe Zusammenhänge auch für Forschende außerhalb der Informatik verständlich zu machen. Außerdem habe ich den Kontakt zwischen dem AUV-Team meiner Hochschule und dem entsprechenden Team am GEOMAR initiiert, weil ich eine fachlich sinnvolle Verbindung gesehen habe.

Mit Spark, Kafka und vergleichbaren Systemen habe ich bislang keine Produktionserfahrung. Die zugrundeliegenden Konzepte verteilter Verarbeitung kenne ich aus dem Studium und habe mich darüber hinaus autodidaktisch damit beschäftigt. Ich bin überzeugt, dass ich mich in jeden konkreten Stack schnell einarbeiten kann, sobald ich weiß, wofür ich es tue. Ich bringe ein solides Python-Fundament, Erfahrung mit Cloud-Deployment und Linux sowie die Fähigkeit, Systeme strukturiert aufzubauen und wartbar zu halten. Ob als direkter Einstieg oder im Rahmen Ihres Trainingsprogramms, ich bin für beides offen.

Ich würde mich über ein Gespräch freuen.

Mit freundlichen Grüßen
Flemming Reese

---

## Anschreiben B — taod (locker, kompakt, stakeholder-orientiert)
Einsatz bei: Startups, modernen Tech-Firmen, flachen Hierarchien, Du-Kultur.

Hallo taod Team,

ich bewerbe mich als Junior Data Engineer bei euch. Der Mix aus Cloud-Datenprojekten, Datenintegration und Kundenarbeit passt sehr gut zu dem, was ich in den letzten Projekten gemacht habe und worauf ich mich weiter spezialisieren möchte: robuste Datenprozesse bauen, die im Alltag wirklich funktionieren – und das im Austausch mit Stakeholdern.

Ich studiere derzeit im M.Sc. Data Science (vsl. Abschluss 02/2026) und bringe praktische Erfahrung aus Forschung und Projekten mit. Am GEOMAR habe ich u. a. einen Test- und Auswertungsprozess in Python automatisiert und in reproduzierbare Abläufe überführt. Wichtig war mir dabei, dass Ergebnisse nachvollziehbar, wiederholbar und für Entscheidungs- und Projektkontexte sauber aufbereitet sind.

Sehr greifbar in Richtung Data Engineering ist außerdem ein Projekt, bei dem ich 10–15 Event-Websites scrape, Daten normalisiere und dedupliziere, in einer Datenbank persistiere und anschließend automatisiert in einen Kalender synchronisiere. Das Ganze ist containerisiert und in der Google Cloud deployt. Zusätzlich habe ich ein Kunststoffrecycling-KMU auf Projektbasis bei der Konzeption der Datenpipeline beraten – mit Fokus auf Datenmodellierung, Datenqualität und der Übersetzung zwischen fachlichen Anforderungen und technischer Umsetzung.

An taod reizt mich besonders, dass ich mich in Projekten an modernen Cloud-Datenplattformen (z. B. Azure oder Snowflake) weiterentwickeln kann und gleichzeitig den Workshop-/Consulting-Anteil aktiv mitnehme. Ich wohne in Köln und finde den regelmäßigen Austausch im Büro und im Team für Lernkurve und Qualität der Arbeit einfach sinnvoll.

Ich freue mich, wenn wir uns kennenlernen.

Viele Grüße
Flemming Reese